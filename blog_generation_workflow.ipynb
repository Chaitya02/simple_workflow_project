{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7669edd",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee4a8f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from typing import TypedDict\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65fd182f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425b1cbb",
   "metadata": {},
   "source": [
    "### State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d795db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMState(TypedDict):\n",
    "    topic: str\n",
    "    outline: str\n",
    "    blog: str\n",
    "    score: int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaef1f25",
   "metadata": {},
   "source": [
    "### Node Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b3b1b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OutlineWorkflow(state: LLMState) -> LLMState:\n",
    "    model = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        temperature=0.5\n",
    "    )\n",
    "    prompt = f\"Write a detailed outline for the topic: {state['topic']}\"\n",
    "    outline = model.invoke(prompt)\n",
    "    state[\"outline\"] = outline.content\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0074066b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BlogWorkflow(state: LLMState) -> LLMState:\n",
    "    model = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        temperature=0.5\n",
    "    )\n",
    "    prompt = f\"Write a blog for the topic: {state['topic']} \\n Outline:{state['outline']}\"\n",
    "    blog = model.invoke(prompt)\n",
    "    state[\"blog\"] = blog.content\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f0b5edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EvaluateWorkflow(state: LLMState) -> LLMState:\n",
    "    model = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        temperature=0.5\n",
    "    )\n",
    "    prompt = f\"Evaluate the blog and give a score from 1-100 for the outline: {state['outline']} \\n Blog: {state['blog']} \\n Only provide the score as an integer.\"\n",
    "    score = model.invoke(prompt)\n",
    "    state[\"score\"] = int(score.content)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62691d75",
   "metadata": {},
   "source": [
    "### Define Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d470d6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(LLMState)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5043595c",
   "metadata": {},
   "source": [
    "### Add Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6edf0e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x126aed6a0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.add_node(\"OutlineWorkflow\", OutlineWorkflow)\n",
    "graph.add_node(\"BlogWorkflow\", BlogWorkflow)\n",
    "graph.add_node(\"EvaluateWorkflow\", EvaluateWorkflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc92c8f",
   "metadata": {},
   "source": [
    "### Add Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fc2aca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x126aed6a0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.add_edge(START, \"OutlineWorkflow\")\n",
    "graph.add_edge(\"OutlineWorkflow\", \"BlogWorkflow\")\n",
    "graph.add_edge(\"BlogWorkflow\", \"EvaluateWorkflow\")\n",
    "graph.add_edge(\"EvaluateWorkflow\", END)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb094d31",
   "metadata": {},
   "source": [
    "### Compile Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c8e9e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = graph.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a159d1",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1e53b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = workflow.invoke({\n",
    "    \"topic\": \"How to use LangGraph for prompt chaining\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04bc30f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(workflow.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d170127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'topic': 'How to use LangGraph for prompt chaining', 'outline': \"## Outline: How to Use LangGraph for Prompt Chaining\\n\\n**I. Introduction (5-10% of content)**\\n\\n   *   **A. What is Prompt Chaining?**\\n        *   1. Definition: Connecting multiple prompts and LLMs to achieve a complex task.\\n        *   2. Benefits:\\n            *   a. Improved accuracy and reliability.\\n            *   b. Ability to handle complex tasks beyond a single prompt.\\n            *   c. Increased modularity and reusability.\\n            *   d. Reduced hallucination.\\n        *   3. Examples of use cases:\\n            *   a. Question answering over large documents.\\n            *   b. Code generation with iterative refinement.\\n            *   c. Dialogue agents with memory and planning.\\n            *   d. Data analysis and report generation.\\n\\n   *   **B. Introduction to LangGraph**\\n        *   1. What is LangGraph?\\n            *   a. A Python library for building stateful, multi-actor applications using LLMs and other tools.\\n            *   b. Built on top of LangChain, providing a graph-based execution framework.\\n        *   2. Key Concepts:\\n            *   a. Nodes: Represent individual units of computation (e.g., LLM calls, tool use).\\n            *   b. Edges: Define the flow of data and control between nodes.\\n            *   c. State:  Maintains information across multiple steps in the graph.\\n            *   d. Graph: The overall structure connecting nodes and edges.\\n        *   3. Why use LangGraph for prompt chaining?\\n            *   a. Simplified workflow orchestration.\\n            *   b. Enhanced state management.\\n            *   c. Built-in support for loops and conditional branching.\\n            *   d. Improved observability and debugging.\\n\\n   *   **C. Overview of the Outline**\\n        *   1. Briefly describe the topics that will be covered in the outline.\\n        *   2. Emphasize the practical, hands-on approach.\\n\\n**II. Setting Up Your Environment (5-10% of content)**\\n\\n   *   **A. Prerequisites**\\n        *   1. Python installation (version requirement).\\n        *   2. Package manager (pip or conda).\\n        *   3. OpenAI API key (or access to other LLM providers).\\n\\n   *   **B. Installation**\\n        *   1. Installing LangChain: `pip install langchain`\\n        *   2. Installing LangGraph: `pip install langgraph`\\n        *   3. Installing necessary dependencies (e.g., `openai`, `tiktoken`).\\n\\n   *   **C. API Key Configuration**\\n        *   1. Setting environment variables for API keys (e.g., `OPENAI_API_KEY`).\\n        *   2. Alternative methods for API key management (e.g., `.env` files).\\n        *   3. Security considerations for API key storage.\\n\\n   *   **D. Verification**\\n        *   1. Simple code snippet to verify successful installation and API key configuration.\\n        *   2. Troubleshooting common installation issues.\\n\\n**III. Building a Simple Prompt Chain with LangGraph (20-25% of content)**\\n\\n   *   **A. Defining Nodes**\\n        *   1. Creating a basic LLM node:\\n            *   a. Using `ChatOpenAI` or other LLM classes from LangChain.\\n            *   b. Defining the prompt template using `PromptTemplate`.\\n            *   c. Passing input variables to the prompt.\\n        *   2. Creating a function node:\\n            *   a. Defining a Python function to process the output of a previous node.\\n            *   b. Integrating the function into the LangGraph.\\n            *   c. Examples: data cleaning, formatting, filtering.\\n\\n   *   **B. Defining Edges**\\n        *   1. Connecting nodes with `add_edge()`:\\n            *   a. Specifying the source and destination nodes.\\n            *   b. Understanding the data flow between nodes.\\n        *   2. Types of edges:\\n            *   a. Direct edges: Simple sequential flow.\\n            *   b. Conditional edges: Branching based on a condition.\\n            *   c. Loopback edges: Creating iterative processes.\\n\\n   *   **C. Defining State**\\n        *   1. Creating a state object:\\n            *   a. Defining the data structure to hold information across steps.\\n            *   b. Examples: conversation history, intermediate results, flags.\\n        *   2. Updating the state:\\n            *   a. Accessing and modifying the state within nodes.\\n            *   b. Using `StateGraph` to manage the state.\\n\\n   *   **D. Creating the Graph**\\n        *   1. Instantiating the `Graph` object.\\n        *   2. Adding nodes and edges to the graph.\\n        *   3. Setting the entry point of the graph.\\n\\n   *   **E. Running the Graph**\\n        *   1. Calling the `compile()` method to prepare the graph for execution.\\n        *   2. Calling the `stream()` or `invoke()` method to run the graph.\\n        *   3. Passing initial input to the graph.\\n        *   4. Retrieving the final output and state.\\n\\n   *   **F. Example: Sentiment Analysis and Response Generation**\\n        *   1. Node 1: Sentiment analysis of user input.\\n        *   2. Node 2: Generate a response based on the sentiment.\\n        *   3. State: Store the conversation history.\\n        *   4. Demonstrating the complete workflow.\\n\\n**IV. Advanced Prompt Chaining Techniques (20-25% of content)**\\n\\n   *   **A. Conditional Branching**\\n        *   1. Using conditional edges to control the flow based on specific conditions.\\n        *   2. Implementing decision-making logic within the graph.\\n        *   3. Example: Routing to different nodes based on user intent (e.g., question, request, complaint).\\n\\n   *   **B. Looping and Iteration**\\n        *   1. Creating loopback edges to repeat a process multiple times.\\n        *   2. Implementing iterative refinement techniques.\\n        *   3. Example: Summarizing a large document by iteratively summarizing smaller chunks.\\n\\n   *   **C. Using Tools**\\n        *   1. Integrating LangChain tools into the LangGraph.\\n        *   2. Enabling the LLM to interact with external resources (e.g., search engines, databases).\\n        *   3. Example: Answering questions using a search engine and a knowledge base.\\n\\n   *   **D. Memory Management**\\n        *   1. Implementing conversation history and context awareness.\\n        *   2. Using different memory types (e.g., buffer memory, summary memory).\\n        *   3. Example: Building a chatbot that remembers previous interactions.\\n\\n   *   **E. Parallel Execution (if supported)**\\n        *   1. Running multiple nodes concurrently to improve performance.\\n        *   2. Managing dependencies and data synchronization.\\n        *   3. Example: Performing multiple independent tasks in parallel.\\n\\n**V. Debugging and Observability (10-15% of content)**\\n\\n   *   **A. Logging and Monitoring**\\n        *   1. Implementing logging to track the execution of the graph.\\n        *   2. Monitoring the performance of individual nodes.\\n        *   3. Using LangChain callbacks for tracing.\\n\\n   *   **B. Error Handling**\\n        *   1. Implementing error handling mechanisms to gracefully handle exceptions.\\n        *   2. Retrying failed nodes.\\n        *   3. Providing informative error messages to the user.\\n\\n   *   **C. Visualizing the Graph**\\n        *   1. Using graph visualization tools to understand the structure and flow of the graph.\\n        *   2. Identifying potential bottlenecks and inefficiencies.\\n        *   3. LangChain's built-in visualization tools.\\n\\n   *   **D. Testing**\\n        *   1. Writing unit tests for individual nodes.\\n        *   2. Writing integration tests for the entire graph.\\n        *   3. Ensuring the graph behaves as expected under different conditions.\\n\\n**VI. Best Practices and Tips (5-10% of content)**\\n\\n   *   **A. Modularity and Reusability**\\n        *   1. Designing nodes and edges that can be easily reused in different graphs.\\n        *   2. Creating reusable components for common tasks.\\n\\n   *   **B. Prompt Engineering**\\n        *   1. Crafting effective prompts that elicit the desired behavior from the LLM.\\n        *   2. Experimenting with different prompt templates and strategies.\\n\\n   *   **C. Security Considerations**\\n        *   1. Protecting against prompt injection attacks.\\n        *   2. Sanitizing user input.\\n        *   3. Securely managing API keys and other sensitive information.\\n\\n   *   **D. Performance Optimization**\\n        *   1. Caching LLM responses to reduce API calls.\\n        *   2. Optimizing the graph structure for efficient execution.\\n        *   3. Using asynchronous operations.\\n\\n**VII. Conclusion (5-10% of content)**\\n\\n   *   **A. Summary of Key Concepts**\\n        *   1. Recap the benefits of using LangGraph for prompt chaining.\\n        *   2. Reiterate the key concepts: nodes, edges, state, graph.\\n\\n   *   **B. Future Directions**\\n        *   1. Briefly discuss potential future developments in LangGraph and prompt chaining.\\n        *   2. Mention advanced topics like multi-agent systems and complex workflow orchestration.\\n\\n   *   **C. Resources and Further Learning**\\n        *   1. Links to LangChain and LangGraph documentation.\\n        *   2. Links to relevant blog posts, tutorials, and examples.\\n        *   3. Pointers to the LangChain community and forums.\\n\\nThis detailed outline provides a comprehensive guide to using LangGraph for prompt chaining, covering everything from basic setup to advanced techniques and best practices.  It emphasizes practical examples and hands-on exercises to help readers quickly learn and apply these concepts.\", 'blog': '## Unleash the Power of Prompt Chaining with LangGraph: A Comprehensive Guide\\n\\nThe world of Large Language Models (LLMs) is constantly evolving, and as we push the boundaries of what these models can do, we often encounter tasks that are too complex for a single prompt. This is where **prompt chaining** comes in – the art of connecting multiple prompts and LLMs to achieve a complex goal. And to orchestrate these intricate workflows, **LangGraph** emerges as a powerful tool.\\n\\nThis blog post will guide you through the process of using LangGraph for prompt chaining, from setting up your environment to implementing advanced techniques. We\\'ll focus on practical examples and hands-on exercises to help you quickly learn and apply these concepts.\\n\\n**I. Introduction: Why Prompt Chaining and LangGraph?**\\n\\n**A. What is Prompt Chaining?**\\n\\nPrompt chaining is the technique of connecting multiple prompts and LLMs together to accomplish a more complex task than a single prompt could handle. Think of it as building a pipeline where the output of one prompt feeds into the input of another.\\n\\n**Benefits of Prompt Chaining:**\\n\\n*   **Improved Accuracy and Reliability:** Breaking down complex tasks into smaller, more manageable steps allows for better control and reduces the likelihood of errors.\\n*   **Ability to Handle Complex Tasks:** Tasks like document summarization, code generation with iterative refinement, and complex data analysis become achievable.\\n*   **Increased Modularity and Reusability:** Individual prompts can be designed as modular components that can be reused in different chains.\\n*   **Reduced Hallucination:** By guiding the LLM through a structured process, we can minimize the risk of generating inaccurate or fabricated information.\\n\\n**Examples of Use Cases:**\\n\\n*   **Question Answering over Large Documents:** Break down a large document into smaller chunks, summarize each chunk, and then answer questions based on the summaries.\\n*   **Code Generation with Iterative Refinement:** Generate initial code, then use feedback prompts to refine and improve the code.\\n*   **Dialogue Agents with Memory and Planning:** Build chatbots that remember previous interactions and plan their responses based on the conversation history.\\n*   **Data Analysis and Report Generation:** Extract data from various sources, analyze it, and generate a comprehensive report.\\n\\n**B. Introduction to LangGraph**\\n\\nLangGraph is a Python library built on top of LangChain that provides a graph-based execution framework for building stateful, multi-actor applications using LLMs and other tools.\\n\\n**Key Concepts:**\\n\\n*   **Nodes:** Represent individual units of computation. These can be LLM calls, tool use, or any other Python function.\\n*   **Edges:** Define the flow of data and control between nodes. They determine the order in which nodes are executed.\\n*   **State:** Maintains information across multiple steps in the graph. This allows you to track conversation history, intermediate results, and other relevant data.\\n*   **Graph:** The overall structure connecting nodes and edges, defining the entire workflow.\\n\\n**Why use LangGraph for prompt chaining?**\\n\\n*   **Simplified Workflow Orchestration:** LangGraph provides a clean and intuitive way to define complex workflows.\\n*   **Enhanced State Management:** Easily manage and update state across multiple steps in the chain.\\n*   **Built-in Support for Loops and Conditional Branching:** Implement complex logic with ease.\\n*   **Improved Observability and Debugging:** Track the execution of your graph and identify potential issues.\\n\\n**C. Overview of the Outline**\\n\\nIn this blog post, we\\'ll cover the following topics:\\n\\n*   Setting up your environment for LangGraph development.\\n*   Building a simple prompt chain with LangGraph.\\n*   Exploring advanced prompt chaining techniques like conditional branching, looping, and tool integration.\\n*   Debugging and observability techniques for LangGraph applications.\\n*   Best practices and tips for building robust and efficient prompt chains.\\n\\nLet\\'s dive in!\\n\\n**II. Setting Up Your Environment**\\n\\n**A. Prerequisites**\\n\\n*   **Python:** Make sure you have Python 3.8 or higher installed.\\n*   **Package Manager:** You\\'ll need either `pip` or `conda` to install the necessary packages.\\n*   **OpenAI API Key:** You\\'ll need an OpenAI API key to use the OpenAI models.  You can obtain one from [OpenAI\\'s website](https://platform.openai.com/).\\n\\n**B. Installation**\\n\\nOpen your terminal and run the following commands to install LangChain, LangGraph, and other necessary dependencies:\\n\\n```bash\\npip install langchain\\npip install langgraph\\npip install openai tiktoken\\n```\\n\\n**C. API Key Configuration**\\n\\nThe easiest way to configure your OpenAI API key is to set it as an environment variable:\\n\\n```bash\\nexport OPENAI_API_KEY=\"YOUR_OPENAI_API_KEY\"\\n```\\n\\nReplace `\"YOUR_OPENAI_API_KEY\"` with your actual API key.\\n\\nAlternatively, you can use a `.env` file to manage your API keys. Create a file named `.env` in your project directory and add the following line:\\n\\n```\\nOPENAI_API_KEY=\"YOUR_OPENAI_API_KEY\"\\n```\\n\\nThen, install the `python-dotenv` package:\\n\\n```bash\\npip install python-dotenv\\n```\\n\\nAnd load the environment variables in your Python code:\\n\\n```python\\nfrom dotenv import load_dotenv\\nimport os\\n\\nload_dotenv()\\nopenai_api_key = os.getenv(\"OPENAI_API_KEY\")\\n```\\n\\n**Security Considerations:** Never commit your API keys directly to your code repository. Use environment variables or a secure secrets management system instead.\\n\\n**D. Verification**\\n\\nTo verify that everything is set up correctly, run the following code snippet:\\n\\n```python\\nimport os\\nfrom langchain_openai import ChatOpenAI\\n\\nopenai_api_key = os.getenv(\"OPENAI_API_KEY\")\\n\\nif not openai_api_key:\\n    print(\"Error: OpenAI API key not found. Please set the OPENAI_API_KEY environment variable.\")\\nelse:\\n    llm = ChatOpenAI(openai_api_key=openai_api_key, model_name=\"gpt-3.5-turbo\")\\n    try:\\n        response = llm.invoke(\"Hello, LangGraph!\")\\n        print(f\"Successfully connected to OpenAI! Response: {response.content}\")\\n    except Exception as e:\\n        print(f\"Error connecting to OpenAI: {e}\")\\n```\\n\\nIf you see a response from OpenAI, you\\'re all set! If you encounter any errors, double-check your API key and installation steps.\\n\\n**III. Building a Simple Prompt Chain with LangGraph**\\n\\nNow that we have our environment set up, let\\'s build a simple prompt chain using LangGraph.  We\\'ll create a chain that analyzes the sentiment of a user\\'s input and then generates a response based on that sentiment.\\n\\n**A. Defining Nodes**\\n\\nFirst, we need to define the nodes in our graph. We\\'ll create two nodes:\\n\\n*   **Sentiment Analysis Node:** This node will analyze the sentiment of the user\\'s input using an LLM.\\n*   **Response Generation Node:** This node will generate a response based on the sentiment analysis.\\n\\n```python\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_openai import ChatOpenAI\\nfrom typing import TypedDict, Dict\\n\\n# Define the state of the graph\\nclass GraphState(TypedDict):\\n    user_input: str\\n    sentiment: str\\n    response: str\\n\\n# Sentiment Analysis Node\\ndef analyze_sentiment(state: GraphState) -> Dict[str, str]:\\n    prompt_template = ChatPromptTemplate.from_template(\\n        \"Analyze the sentiment of the following text: {text}.  Respond with only \\'positive\\', \\'negative\\', or \\'neutral\\'.\"\\n    )\\n    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\\n    chain = prompt_template | llm\\n    sentiment = chain.invoke({\"text\": state[\\'user_input\\']}).content\\n    print(f\"Sentiment: {sentiment}\")\\n    return {\"sentiment\": sentiment}\\n\\n# Response Generation Node\\ndef generate_response(state: GraphState) -> Dict[str, str]:\\n    prompt_template = ChatPromptTemplate.from_template(\\n        \"\"\"You are a helpful assistant.  The user\\'s sentiment is {sentiment}.\\n        Respond to the user\\'s input: {user_input} in a way that is appropriate for the sentiment.\"\"\"\\n    )\\n    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\\n    chain = prompt_template | llm\\n    response = chain.invoke({\"sentiment\": state[\\'sentiment\\'], \"user_input\": state[\\'user_input\\']}).content\\n    print(f\"Response: {response}\")\\n    return {\"response\": response}\\n```\\n\\nIn this code:\\n\\n*   We define a `GraphState` TypedDict to hold the state of our graph. This includes the user input, the sentiment analysis, and the generated response.\\n*   We use `ChatPromptTemplate` to define the prompts for each node.\\n*   We use `ChatOpenAI` to create an LLM instance.\\n*   Each node function takes the current `state` as input and returns a dictionary containing the updated values for the state.\\n\\n**B. Defining Edges**\\n\\nNext, we need to define the edges that connect the nodes. In this simple example, we\\'ll create a direct edge from the sentiment analysis node to the response generation node.\\n\\n```python\\nfrom langgraph.graph import StateGraph, END\\n\\n# Create a new graph\\nworkflow = StateGraph(GraphState)\\n\\n# Add the nodes\\nworkflow.add_node(\"analyze_sentiment\", analyze_sentiment)\\nworkflow.add_node(\"generate_response\", generate_response)\\n\\n# Add the edges\\nworkflow.add_edge(\"analyze_sentiment\", \"generate_response\")\\nworkflow.set_entry_point(\"analyze_sentiment\")\\nworkflow.add_edge(\"generate_response\", END)\\n```\\n\\nHere, we use the `StateGraph` class to create our graph. We add the nodes using `add_node()` and the edges using `add_edge()`. We also set the entry point of the graph to the sentiment analysis node using `set_entry_point()`.  `END` signifies the end of the graph execution.\\n\\n**C. Defining State**\\n\\nWe already defined the `GraphState` TypedDict in the previous step. This defines the structure of the state that will be passed between nodes.\\n\\n**D. Creating the Graph**\\n\\nWe already created the graph in the previous step using `workflow = StateGraph(GraphState)`.\\n\\n**E. Running the Graph**\\n\\nNow, let\\'s run the graph with some user input:\\n\\n```python\\nfrom langchain_core.utils.function_utils import is_async_callable\\n# Compile the graph\\nchain = workflow.compile()\\n\\n# Run the graph\\ninputs = {\"user_input\": \"I\\'m feeling really happy today!\"}\\nresult = chain.invoke(inputs)\\n\\n# Print the final result\\nprint(f\"Final Response: {result[\\'response\\']}\")\\n```\\n\\nThis code compiles the graph using `compile()` and then runs it with the input \"I\\'m feeling really happy today!\". The final response from the graph will be printed to the console.\\n\\n**F. Example: Sentiment Analysis and Response Generation (Complete Code)**\\n\\nHere\\'s the complete code for this example:\\n\\n```python\\nimport os\\nfrom dotenv import load_dotenv\\nfrom typing import TypedDict, Dict\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_openai import ChatOpenAI\\nfrom langgraph.graph import StateGraph, END\\n\\nload_dotenv()\\nopenai_api_key = os.getenv(\"OPENAI_API_KEY\")\\n\\n# Define the state of the graph\\nclass GraphState(TypedDict):\\n    user_input: str\\n    sentiment: str\\n    response: str\\n\\n# Sentiment Analysis Node\\ndef analyze_sentiment(state: GraphState) -> Dict[str, str]:\\n    prompt_template = ChatPromptTemplate.from_template(\\n        \"Analyze the sentiment of the following text: {text}.  Respond with only \\'positive\\', \\'negative\\', or \\'neutral\\'.\"\\n    )\\n    llm = ChatOpenAI(openai_api_key=openai_api_key, model_name=\"gpt-3.5-turbo\")\\n    chain = prompt_template | llm\\n    sentiment = chain.invoke({\"text\": state[\\'user_input\\']}).content\\n    print(f\"Sentiment: {sentiment}\")\\n    return {\"sentiment\": sentiment}\\n\\n# Response Generation Node\\ndef generate_response(state: GraphState) -> Dict[str, str]:\\n    prompt_template = ChatPromptTemplate.from_template(\\n        \"\"\"You are a helpful assistant.  The user\\'s sentiment is {sentiment}.\\n        Respond to the user\\'s input: {user_input} in a way that is appropriate for the sentiment.\"\"\"\\n    )\\n    llm = ChatOpenAI(openai_api_key=openai_api_key, model_name=\"gpt-3.5-turbo\")\\n    chain = prompt_template | llm\\n    response = chain.invoke({\"sentiment\": state[\\'sentiment\\'], \"user_input\": state[\\'user_input\\']}).content\\n    print(f\"Response: {response}\")\\n    return {\"response\": response}\\n\\n# Create a new graph\\nworkflow = StateGraph(GraphState)\\n\\n# Add the nodes\\nworkflow.add_node(\"analyze_sentiment\", analyze_sentiment)\\nworkflow.add_node(\"generate_response\", generate_response)\\n\\n# Add the edges\\nworkflow.add_edge(\"analyze_sentiment\", \"generate_response\")\\nworkflow.set_entry_point(\"analyze_sentiment\")\\nworkflow.add_edge(\"generate_response\", END)\\n\\n# Compile the graph\\nchain = workflow.compile()\\n\\n# Run the graph\\ninputs = {\"user_input\": \"I\\'m feeling really happy today!\"}\\nresult = chain.invoke(inputs)\\n\\n# Print the final result\\nprint(f\"Final Response: {result[\\'response\\']}\")\\n```\\n\\n**IV. Advanced Prompt Chaining Techniques**\\n\\nNow that we\\'ve built a simple prompt chain, let\\'s explore some advanced techniques that can make your LangGraph applications even more powerful.\\n\\n**A. Conditional Branching**\\n\\nConditional branching allows you to control the flow of the graph based on specific conditions. For example, you might want to route the user to different nodes based on their intent.\\n\\n```python\\nfrom langgraph.graph import StateGraph, END\\nfrom typing import TypedDict, Dict\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_openai import ChatOpenAI\\nimport os\\nfrom dotenv import load_dotenv\\n\\nload_dotenv()\\nopenai_api_key = os.getenv(\"OPENAI_API_KEY\")\\n\\n# Define the state of the graph\\nclass GraphState(TypedDict):\\n    user_input: str\\n    intent: str\\n    response: str\\n\\n# Intent Recognition Node\\ndef recognize_intent(state: GraphState) -> Dict[str, str]:\\n    prompt_template = ChatPromptTemplate.from_template(\\n        \"What is the user\\'s intent?  Respond with only \\'question\\', \\'request\\', or \\'complaint\\'. User input: {text}\"\\n    )\\n    llm = ChatOpenAI(openai_api_key=openai_api_key, model_name=\"gpt-3.5-turbo\")\\n    chain = prompt_template | llm\\n    intent = chain.invoke({\"text\": state[\\'user_input\\']}).content\\n    print(f\"Intent: {intent}\")\\n    return {\"intent\": intent}\\n\\n# Question Answering Node\\ndef answer_question(state: GraphState) -> Dict[str, str]:\\n    prompt_template = ChatPromptTemplate.from_template(\\n        \"Answer the user\\'s question: {user_input}\"\\n    )\\n    llm = ChatOpenAI(openai_api_key=openai_api_key, model_name=\"gpt-3.5-turbo\")\\n    chain = prompt_template | llm\\n    response = chain.invoke({\"user_input\": state[\\'user_input\\']}).content\\n    print(f\"Answer: {response}\")\\n    return {\"response\": response}\\n\\n# Handle Request Node\\ndef handle_request(state: GraphState) -> Dict[str, str]:\\n    prompt_template = ChatPromptTemplate.from_template(\\n        \"Fulfill the user\\'s request: {user_input}\"\\n    )\\n    llm = ChatOpenAI(openai_api_key=openai_api_key, model_name=\"gpt-3.5-turbo\")\\n    chain = prompt_template | llm\\n    response = chain.invoke({\"user_input\": state[\\'user_input\\']}).content\\n    print(f\"Request Response: {response}\")\\n    return {\"response\": response}\\n\\n# Handle Complaint Node\\ndef handle_complaint(state: GraphState) -> Dict[str, str]:\\n    prompt_template = ChatPromptTemplate.from_template(\\n        \"Acknowledge and address the user\\'s complaint: {user_input}\"\\n    )\\n    llm = ChatOpenAI(openai_api_key=openai_api_key, model_name=\"gpt-3.5-turbo\")\\n    chain = prompt_template | llm\\n    response = chain.invoke({\"user_input\": state[\\'user_input\\']}).content\\n    print(f\"Complaint Response: {response}\")\\n    return {\"response\": response}\\n\\n# Define a function to determine the next node based on the intent\\ndef route(state: GraphState):\\n    if state[\\'intent\\'] == \"question\":\\n        return \"answer_question\"\\n    elif state[\\'intent\\'] == \"request\":\\n        return \"handle_request\"\\n    elif state[\\'intent\\'] == \"complaint\":\\n        return \"handle_complaint\"\\n    else:\\n        return END\\n\\n# Create a new graph\\nworkflow = StateGraph(GraphState)\\n\\n# Add the nodes\\nworkflow.add_node(\"recognize_intent\", recognize_intent)\\nworkflow.add_node(\"answer_question\", answer_question)\\nworkflow.add_node(\"handle_request\", handle_request)\\nworkflow.add_node(\"handle_complaint\", handle_complaint)\\n\\n# Add the edges\\nworkflow.add_edge(\"recognize_intent\", route)\\nworkflow.add_conditional_edges(\\n    \"recognize_intent\",\\n    route,\\n    {\\n        \"question\": \"answer_question\",\\n        \"request\": \"handle_request\",\\n        \"complaint\": \"handle_complaint\",\\n    }\\n)\\nworkflow.add_edge(\"answer_question\", END)\\nworkflow.add_edge(\"handle_request\", END)\\nworkflow.add_edge(\"handle_complaint\", END)\\n\\nworkflow.set_entry_point(\"recognize_intent\")\\n\\n# Compile the graph\\nchain = workflow.compile()\\n\\n# Run the graph\\ninputs = {\"user_input\": \"I need help resetting my password.\"}\\nresult = chain.invoke(inputs)\\n\\n# Print the final result\\nprint(f\"Final Response: {result[\\'response\\']}\")\\n```\\n\\nIn this example:\\n\\n*   We added an `intent` field to the `GraphState` to store the user\\'s intent.\\n*   We created an `recognize_intent` node to determine the user\\'s intent.\\n*   We created three separate nodes to handle questions, requests, and complaints.\\n*   We used `workflow.add_conditional_edges` to route the user to the appropriate node based on their intent.\\n\\n**B. Looping and Iteration**\\n\\nLooping and iteration allow you to repeat a process multiple times. This is useful for tasks like summarizing large documents or iteratively refining code.\\n\\n```python\\nfrom langgraph.graph import StateGraph, END\\nfrom typing import TypedDict, Dict\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_openai import ChatOpenAI\\nimport os\\nfrom dotenv import load_dotenv\\n\\nload_dotenv()\\nopenai_api_key = os.getenv(\"OPENAI_API_KEY\")\\n\\n# Define the state of the graph\\nclass GraphState(TypedDict):\\n    text: str\\n    summary: str\\n    iteration: int\\n\\n# Summarization Node\\ndef summarize_text(state: GraphState) -> Dict[str, str]:\\n    prompt_template = ChatPromptTemplate.from_template(\\n        \"Summarize the following text: {text}.  The current summary is: {summary}\"\\n    )\\n    llm = ChatOpenAI(openai_api_key=openai_api_key, model_name=\"gpt-3.5-turbo\")\\n    chain = prompt_template | llm\\n    summary = chain.invoke({\"text\": state[\\'text\\'], \"summary\": state[\\'summary\\']}).content\\n    print(f\"Summary: {summary}\")\\n    return {\"summary\": summary}\\n\\n# Check Completion Node\\ndef check_completion(state: GraphState) -> Dict[str, str]:\\n    if state[\\'iteration\\'] >= 3:\\n        return END\\n    else:\\n        return \"summarize_text\"\\n\\n# Increment Iteration Node\\ndef increment_iteration(state: GraphState) -> Dict[str, int]:\\n    iteration = state[\\'iteration\\'] + 1\\n    print(f\"Iteration: {iteration}\")\\n    return {\"iteration\": iteration}\\n\\n# Create a new graph\\nworkflow = StateGraph(GraphState)\\n\\n# Add the nodes\\nworkflow.add_node(\"summarize_text\", summarize_text)\\nworkflow.add_node(\"check_completion\", check_completion)\\nworkflow.add_node(\"increment_iteration\", increment_iteration)\\n\\n# Add the edges\\nworkflow.add_edge(\"summarize_text\", \"increment_iteration\")\\nworkflow.add_edge(\"increment_iteration\", \"check_completion\")\\nworkflow.add_conditional_edges(\\n    \"check_completion\",\\n    check_completion,\\n    {\\n        END: END,\\n        \"summarize_text\": \"summarize_text\",\\n    }\\n)\\nworkflow.set_entry_point(\"summarize_text\")\\n\\n# Compile the graph\\nchain = workflow.compile()\\n\\n# Run the graph\\ninputs = {\"text\": \"This is a long document that needs to be summarized.  It contains a lot of information that is important to understand. The main topic is LangGraph and how to use it for prompt chaining.  It\\'s a powerful tool for building complex applications with LLMs.\", \"summary\": \"\", \"iteration\": 0}\\nresult = chain.invoke(inputs)\\n\\n# Print the final result\\nprint(f\"Final Summary: {result[\\'summary\\']}\")\\n```\\n\\nIn this example:\\n\\n*   We added a `summary` and `iteration` field to the `GraphState`.\\n*   We created a `summarize_text` node to summarize the text.\\n*   We created a `check_completion` node to determine if the summarization is complete.\\n*   We used a loopback edge to repeat the summarization process multiple times.\\n\\n**C. Using Tools**\\n\\nLangGraph can be integrated with LangChain tools to enable the LLM to interact with external resources like search engines and databases.\\n\\n```python\\nfrom langgraph.graph import StateGraph, END\\nfrom typing import TypedDict, Dict\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain.tools import DuckDuckGoSearchRun\\nimport os\\nfrom dotenv import load_dotenv\\n\\nload_dotenv()\\nopenai_api_key = os.getenv(\"OPENAI_API_KEY\")\\n\\n# Define the state of the graph\\nclass GraphState(TypedDict):\\n    user_input: str\\n    search_results: str\\n    response: str\\n\\n# Search Node\\ndef search_internet(state: GraphState) -> Dict[str, str]:\\n    search = DuckDuckGoSearchRun()\\n    search_results = search.run(state[\\'user_input\\'])\\n    print(f\"Search Results: {search_results}\")\\n    return {\"search_results\": search_results}\\n\\n# Response Generation Node\\ndef generate_response(state: GraphState) -> Dict[str, str]:\\n    prompt_template = ChatPromptTemplate.from_template(\\n        \"Answer the user\\'s question using the following search results: {search_results}. User question: {user_input}\"\\n    )\\n    llm = ChatOpenAI(openai_api_key=openai_api_key, model_name=\"gpt-3.5-turbo\")\\n    chain = prompt_template | llm\\n    response = chain.invoke({\"search_results\": state[\\'search_results\\'], \"user_input\": state[\\'user_input\\']}).content\\n    print(f\"Response: {response}\")\\n    return {\"response\": response}\\n\\n# Create a new graph\\nworkflow = StateGraph(GraphState)\\n\\n# Add the nodes\\nworkflow.add_node(\"search_internet\", search_internet)\\nworkflow.add_node(\"generate_response\", generate_response)\\n\\n# Add the edges\\nworkflow.add_edge(\"search_internet\", \"generate_response\")\\nworkflow.set_entry_point(\"search_internet\")\\nworkflow.add_edge(\"generate_response\", END)\\n\\n# Compile the graph\\nchain = workflow.compile()\\n\\n# Run the graph\\ninputs = {\"user_input\": \"What is the capital of France?\"}\\nresult = chain.invoke(inputs)\\n\\n# Print the final result\\nprint(f\"Final Response: {result[\\'response\\']}\")\\n```\\n\\nIn this example:\\n\\n*   We added a `search_results` field to the `GraphState`.\\n*   We created a `search_internet` node to search the internet using the DuckDuckGo search tool.\\n*   We used the search results to generate a response to the user\\'s question.\\n\\n**D. Memory Management**\\n\\nMemory management allows you to maintain conversation history and context awareness in your LangGraph applications. You can use different memory types like buffer memory and summary memory to store and retrieve information from previous interactions. LangChain provides various memory implementations that can be integrated into LangGraph nodes.\\n\\n**E. Parallel Execution**\\n\\nWhile not directly supported in all LangGraph versions, the underlying LangChain framework often allows for asynchronous operations, which can be leveraged to run multiple nodes concurrently, improving performance. This typically involves using `async` and `await` keywords in your node functions and ensuring your LLM provider supports asynchronous requests.  Always check the LangGraph documentation for the most up-to-date information on parallel execution capabilities.\\n\\n**V. Debugging and Observability**\\n\\nDebugging and observability are crucial for building robust and reliable LangGraph applications.\\n\\n**A. Logging and Monitoring**\\n\\nImplement logging to track the execution of the graph and monitor the performance of individual nodes. You can use Python\\'s built-in `logging` module or a more advanced logging framework like `structlog`. LangChain callbacks can also be used for tracing the execution of the graph.\\n\\n**B. Error Handling**\\n\\nImplement error handling mechanisms to gracefully handle exceptions. You can use `try-except` blocks to catch errors and retry failed nodes. Provide informative error messages to the user to help them understand what went wrong.\\n\\n**C. Visualizing the Graph**\\n\\nLangChain provides built-in visualization tools that can help you understand the structure and flow of the graph. You can use these tools to identify potential bottlenecks and inefficiencies.\\n\\n**D. Testing**\\n\\nWrite unit tests for individual nodes and integration tests for the entire graph. Ensure that the graph behaves as expected under different conditions.\\n\\n**VI. Best Practices and Tips**\\n\\nHere are some best practices and tips for building robust and efficient prompt chains with LangGraph:\\n\\n*   **Modularity and Reusability:** Design nodes and edges that can be easily reused in different graphs. Create reusable components for common tasks.\\n*   **Prompt Engineering:** Craft effective prompts that elicit the desired behavior from the LLM. Experiment with different prompt templates and strategies.\\n*   **Security Considerations:** Protect against prompt injection attacks. Sanitize user input. Securely manage API keys and other sensitive information.\\n*   **Performance Optimization:** Cache LLM responses to reduce API calls. Optimize the graph structure for efficient execution. Use asynchronous operations where possible.\\n\\n**VII. Conclusion**\\n\\n**A. Summary of Key Concepts**\\n\\nLangGraph is a powerful tool for building stateful, multi-actor applications using LLMs and other tools. It simplifies workflow orchestration, enhances state management, and provides built-in support for loops and conditional branching. By mastering the key concepts of nodes, edges, state, and graph, you can unlock the full potential of LangGraph for prompt chaining.\\n\\n**B. Future Directions**\\n\\nThe field of prompt chaining is constantly evolving. Future developments in LangGraph and related technologies will likely focus on advanced topics like multi-agent systems, complex workflow orchestration, and improved integration with other tools and services.\\n\\n**C. Resources and Further Learning**\\n\\n*   **LangChain Documentation:** [https://python.langchain.com/](https://python.langchain.com/)\\n*   **LangGraph Documentation:** (Refer to LangChain documentation as LangGraph is integrated within LangChain)\\n*   **LangChain Community:** [https://discord.gg/6adAnGJdrQ](https://discord.gg/6adAnGJdrQ)\\n\\nBy following the guidelines and examples in this blog post, you can start building your own powerful prompt chains with LangGraph and unlock the full potential of LLMs. Happy coding!', 'score': 92}\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ceb05d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
